
@inproceedings{thiede_carbon_2023,
	address = {New York, NY, USA},
	series = {{SoCC} '23},
	title = {Carbon {Containers}: {A} {System}-level {Facility} for {Managing} {Application}-level {Carbon} {Emissions}},
	isbn = {9798400703874},
	shorttitle = {Carbon {Containers}},
	url = {https://dl.acm.org/doi/10.1145/3620678.3624644},
	doi = {10.1145/3620678.3624644},
	abstract = {To reduce their environmental impact, cloud datacenters' are increasingly focused on optimizing applications' carbon-efficiency, or work done per mass of carbon emitted. To facilitate such optimizations, we present Carbon Containers, a simple system-level facility, which extends prior work on power containers, that automatically regulates applications' carbon emissions in response to variations in both their work-load's intensity and their energy's carbon-intensity. Specifically, Carbon Containers enable applications to specify a maximum carbon emissions rate (in g.CO2e/hr), and then transparently enforce this rate via a combination of vertical scaling, container migration, and suspend/resume while maximizing either energy-efficiency or performance. Carbon Containers are especially useful for applications that i) must continue running even during high-carbon periods, and ii) execute in regions with few variations in carbon-intensity. These low-variability regions also tend to have high average carbon-intensity, which increases the importance of regulating carbon emissions. We implement a Carbon Container prototype by extending Linux Containers to incorporate the mechanisms above and evaluate it using real workload traces and carbon-intensity data from multiple regions. We compare Carbon Containers with prior work that regulates carbon emissions by suspending/resuming applications during high/low carbon periods. We show that Carbon Containers are more carbon-efficient and improve performance while maintaining similar carbon emissions.},
	urldate = {2024-01-12},
	booktitle = {Proceedings of the 2023 {ACM} {Symposium} on {Cloud} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Thiede, John and Bashir, Noman and Irwin, David and Shenoy, Prashant},
	month = oct,
	year = {2023},
	keywords = {Carbon-efficiency, energy-efficiency, performance},
	pages = {17--31},
	file = {Thiede et al. - 2023 - Carbon Containers A System-level Facility for Man.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Thiede et al/Thiede et al. - 2023 - Carbon Containers A System-level Facility for Man.pdf:application/pdf},
}

@inproceedings{fuerst_cloud-scale_2020,
	address = {New York, NY, USA},
	series = {{HPDC} '20},
	title = {Cloud-scale {VM}-deflation for {Running} {Interactive} {Applications} {On} {Transient} {Servers}},
	isbn = {978-1-4503-7052-3},
	url = {https://dl.acm.org/doi/10.1145/3369583.3392675},
	doi = {10.1145/3369583.3392675},
	abstract = {Transient computing has become popular in public cloud environments for running delay-insensitive batch and data processing applications at low cost. Since transient cloud servers can be revoked at any time by the cloud provider, they are considered unsuitable for running interactive application such as web services. In this paper, we present VM deflation as an alternative mechanism to server preemption for reclaiming resources from transient cloud servers under resource pressure. Using real traces from top-tier cloud providers, we show the feasibility of using VM deflation as a resource reclamation mechanism for interactive applications in public clouds. We show how current hypervisor mechanisms can be used to implement VM deflation and present cluster deflation policies for resource management of transient and on-demand cloud VMs. Experimental evaluation of our deflation system on a Linux cluster shows that microservice-based applications can be deflated by up to 50\% with negligible performance overhead. Our cluster-level deflation policies allow overcommitment levels as high as 50\%, with less than a 1\% decrease in application throughput, and can enable cloud platforms to increase revenue by 30\%.},
	urldate = {2024-01-12},
	booktitle = {Proceedings of the 29th {International} {Symposium} on {High}-{Performance} {Parallel} and {Distributed} {Computing}},
	publisher = {Association for Computing Machinery},
	author = {Fuerst, Alexander and Ali-Eldin, Ahmed and Shenoy, Prashant and Sharma, Prateek},
	month = jun,
	year = {2020},
	keywords = {cloud computing, data center, deflation, transient computing},
	pages = {53--64},
	file = {Fuerst et al. - 2020 - Cloud-scale VM-deflation for Running Interactive A.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Fuerst et al/Fuerst et al. - 2020 - Cloud-scale VM-deflation for Running Interactive A.pdf:application/pdf},
}

@inproceedings{bai_pipeswitch_2020,
	title = {\{{PipeSwitch}\}: {Fast} {Pipelined} {Context} {Switching} for {Deep} {Learning} {Applications}},
	isbn = {978-1-939133-19-9},
	shorttitle = {\{{PipeSwitch}\}},
	url = {https://www.usenix.org/conference/osdi20/presentation/bai},
	language = {en},
	urldate = {2024-01-13},
	author = {Bai, Zhihao and Zhang, Zhen and Zhu, Yibo and Jin, Xin},
	year = {2020},
	pages = {499--514},
	file = {Bai et al. - 2020 - PipeSwitch Fast Pipelined Context Switching for.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Bai et al/Bai et al. - 2020 - PipeSwitch Fast Pipelined Context Switching for.pdf:application/pdf},
}

@inproceedings{han_microsecond-scale_2022,
	title = {Microsecond-scale {Preemption} for {Concurrent} \{{GPU}-accelerated\} \{{DNN}\} {Inferences}},
	isbn = {978-1-939133-28-1},
	url = {https://www.usenix.org/conference/osdi22/presentation/han},
	language = {en},
	urldate = {2024-01-13},
	author = {Han, Mingcong and Zhang, Hanze and Chen, Rong and Chen, Haibo},
	year = {2022},
	pages = {539--558},
	file = {Han et al. - 2022 - Microsecond-scale Preemption for Concurrent GPU-a.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Han et al/Han et al. - 2022 - Microsecond-scale Preemption for Concurrent GPU-a.pdf:application/pdf},
}

@article{desislavov_trends_2023,
	title = {Trends in {AI} inference energy consumption: {Beyond} the performance-vs-parameter laws of deep learning},
	volume = {38},
	issn = {2210-5379},
	shorttitle = {Trends in {AI} inference energy consumption},
	url = {https://www.sciencedirect.com/science/article/pii/S2210537923000124},
	doi = {10.1016/j.suscom.2023.100857},
	abstract = {The progress of some AI paradigms such as deep learning is said to be linked to an exponential growth in the number of parameters. There are many studies corroborating these trends, but does this translate into an exponential increase in energy consumption? In order to answer this question we focus on inference costs rather than training costs, as the former account for most of the computing effort, solely because of the multiplicative factors. Also, apart from algorithmic innovations, we account for more specific and powerful hardware (leading to higher FLOPS) that is usually accompanied with important energy efficiency optimisations. We also move the focus from the first implementation of a breakthrough paper towards the consolidated version of the techniques one or two year later. Under this distinctive and comprehensive perspective, we analyse relevant models in the areas of computer vision and natural language processing: for a sustained increase in performance we see a much softer growth in energy consumption than previously anticipated. The only caveat is, yet again, the multiplicative factor, as future AI increases penetration and becomes more pervasive.},
	urldate = {2024-01-13},
	journal = {Sustainable Computing: Informatics and Systems},
	author = {Desislavov, Radosvet and Martínez-Plumed, Fernando and Hernández-Orallo, José},
	month = apr,
	year = {2023},
	keywords = {AI progress, Artificial Intelligence, Deep learning, Energy consumption, Inference, Performance analysis, Performance evaluation},
	pages = {100857},
	file = {Desislavov et al. - 2023 - Trends in AI inference energy consumption Beyond .pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Desislavov et al/Desislavov et al. - 2023 - Trends in AI inference energy consumption Beyond .pdf:application/pdf;ScienceDirect Snapshot:/Users/shinwookim/Zotero/storage/ZHXXW549/S2210537923000124.html:text/html},
}

@inproceedings{luo_performance_2011,
	title = {A {Performance} and {Energy} {Consumption} {Analytical} {Model} for {GPU}},
	url = {https://ieeexplore.ieee.org/abstract/document/6118939},
	doi = {10.1109/DASC.2011.117},
	abstract = {Even with a powerful hardware in parallel execution, it is still difficult to improve the application performance and reduce energy consumption without realizing the performance bottlenecks of parallel programs on GPU architectures. To help programmers have a better insight into the performance and energy-saving bottleneck of parallel applications on GPU architectures, we propose two models: an execution time prediction model and an energy consumption prediction model. The execution time prediction model(ETPM) can estimate the execution time of massively parallel programs which take the instruction-level and thread-level parallelism into consideration. ETPM contains two components: memory sub-model and computation sub-model. The memory sub-model is estimating the cost of memory instructions by considering the number of active threads and GPU memory bandwidth. Correspondingly, the computation sub-model is estimating the cost of computation instructions by considering the number of active threads and the application's arithmetic intensity. We use ocelot to analysis PTX codes to obtain several input parameters for the two sub-models such as the memory transaction number and data size. Basing on the two sub-models, the analytical model can estimates the cost of each instruction while considering instruction-level and thread-level parallelism, thereby estimating the overall execution time of an application. The energy consumption prediction model(ECPM) can estimate the total energy consumption basing on the data from ETPM. We compare the outcome from the models and the actual execution on GTX260 and Tesla C2050. The results show that the models can reach almost 90 percentage accuracy in average for the benchmarks we used.},
	urldate = {2024-01-13},
	booktitle = {2011 {IEEE} {Ninth} {International} {Conference} on {Dependable}, {Autonomic} and {Secure} {Computing}},
	author = {Luo, Cheng and Suda, Reiji},
	month = dec,
	year = {2011},
	pages = {658--665},
	file = {IEEE Xplore Abstract Record:/Users/shinwookim/Zotero/storage/V8C3QUUM/6118939.html:text/html;Luo and Suda - 2011 - A Performance and Energy Consumption Analytical Mo.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Luo_Suda/Luo and Suda - 2011 - A Performance and Energy Consumption Analytical Mo.pdf:application/pdf},
}

@inproceedings{wang_benchmarking_2020,
	title = {Benchmarking the {Performance} and {Energy} {Efficiency} of {AI} {Accelerators} for {AI} {Training}},
	url = {https://ieeexplore.ieee.org/abstract/document/9139681},
	doi = {10.1109/CCGrid49817.2020.00-15},
	abstract = {Deep learning has become widely used in complex AI applications. Yet, training a deep neural network (DNNs) model requires a considerable amount of calculations, long running time, and much energy. Nowadays, many-core AI accelerators (e.g., GPUs and TPUs) are designed to improve the performance of AI training. However, processors from different vendors perform dissimilarly in terms of performance and energy consumption. To investigate the differences among several popular off-the-shelf processors (i.e., Intel CPU, NVIDIA GPU, AMD GPU, and Google TPU) in training DNNs, we carry out a comprehensive empirical study on the performance and energy efficiency of these processors1 by benchmarking a representative set of deep learning workloads, including computation-intensive operations, classical convolutional neural networks (CNNs), recurrent neural networks (LSTM), Deep Speech 2, and Transformer. Different from the existing end-to-end benchmarks which only present the training time, We try to investigate the impact of hardware, vendor's software library, and deep learning framework on the performance and energy consumption of AI training. Our evaluation methods and results not only provide an informative guide for end users to select proper AI accelerators, but also expose some opportunities for the hardware vendors to improve their software library.},
	urldate = {2024-01-13},
	booktitle = {2020 20th {IEEE}/{ACM} {International} {Symposium} on {Cluster}, {Cloud} and {Internet} {Computing} ({CCGRID})},
	author = {Wang, Yuxin and Wang, Qiang and Shi, Shaohuai and He, Xin and Tang, Zhenheng and Zhao, Kaiyong and Chu, Xiaowen},
	month = may,
	year = {2020},
	pages = {744--751},
	file = {IEEE Xplore Abstract Record:/Users/shinwookim/Zotero/storage/Q9W3K3B8/9139681.html:text/html;Wang et al. - 2020 - Benchmarking the Performance and Energy Efficiency.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Wang et al/Wang et al. - 2020 - Benchmarking the Performance and Energy Efficiency.pdf:application/pdf},
}

@article{yokoyama_investigating_2023,
	title = {Investigating hardware and software aspects in the energy consumption of machine learning: {A} green {AI}-centric analysis},
	volume = {35},
	issn = {1532-0634},
	shorttitle = {Investigating hardware and software aspects in the energy consumption of machine learning},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.7825},
	doi = {10.1002/cpe.7825},
	abstract = {Much has been discussed about artificial intelligence's negative environmental impacts due to its power-hungry Machine Learning algorithms and CO2{\textbackslash} CO\_2 {\textbackslash} emissions linked to this. This work discusses three direct impacts of AI on energy consumption associated with computation: the software, the hardware, and the energy source's carbon intensity. We present an up-to-date revision of the literature and assess it through experiments. For hardware, we evaluate the use of ARM-based single-board computers for training Machine Learning algorithms. An experimental setup was developed training the algorithm XGBoost and its cost-effectiveness (energy consumption, acquisition cost, and execution time) compared with the X86-64 and GPU architectures and other algorithms. In addition, the CO2{\textbackslash} CO\_2 {\textbackslash} is estimated for these experiments and compared for three energy sources. The results show that this type of architecture can become a viable and greener alternative, not only for inference but also for training these algorithms. Finally, we evaluated low precision for training Random Forest algorithms with different datasets for the software aspect. Results show that is possible energy reduction with no decrease in accuracy.},
	language = {en},
	number = {24},
	urldate = {2024-01-13},
	journal = {Concurrency and Computation: Practice and Experience},
	author = {Yokoyama, André M. and Ferro, Mariza and de Paula, Felipe B. and Vieira, Vitor G. and Schulze, Bruno},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.7825},
	keywords = {algorithm, ARM, artificial intelligence, CO2\$ CO\_2 \$, energy, green AI, machine learning},
	pages = {e7825},
	file = {Full Text PDF:/Users/shinwookim/Zotero/storage/Y7EQIR7S/Yokoyama et al. - 2023 - Investigating hardware and software aspects in the.pdf:application/pdf;Snapshot:/Users/shinwookim/Zotero/storage/G5NCJLUU/cpe.html:text/html;Yokoyama et al. - 2023 - Investigating hardware and software aspects in the.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Yokoyama et al/Yokoyama et al. - 2023 - Investigating hardware and software aspects in the.pdf:application/pdf},
}

@article{tanasic_enabling_2014,
	title = {Enabling preemptive multiprogramming on {GPUs}},
	volume = {42},
	issn = {0163-5964},
	url = {https://dl.acm.org/doi/10.1145/2678373.2665702},
	doi = {10.1145/2678373.2665702},
	abstract = {GPUs are being increasingly adopted as compute accelerators in many domains, spanning environments from mobile systems to cloud computing. These systems are usually running multiple applications, from one or several users. However GPUs do not provide the support for resource sharing traditionally expected in these scenarios. Thus, such systems are unable to provide key multiprogrammed workload requirements, such as responsiveness, fairness or quality of service. In this paper, we propose a set of hardware extensions that allow GPUs to efficiently support multiprogrammed GPU workloads. We argue for preemptive multitasking and design two preemption mechanisms that can be used to implement GPU scheduling policies. We extend the architecture to allow concurrent execution of GPU kernels from different user processes and implement a scheduling policy that dynamically distributes the GPU cores among concurrently running kernels, according to their priorities. We extend the NVIDIA GK110 (Kepler) like GPU architecture with our proposals and evaluate them on a set of multiprogrammed workloads with up to eight concurrent processes. Our proposals improve execution time of high-priority processes by 15.6x, the average application turnaround time between 1.5x to 2x, and system fairness up to 3.4x},
	number = {3},
	urldate = {2024-01-17},
	journal = {ACM SIGARCH Computer Architecture News},
	author = {Tanasic, Ivan and Gelado, Isaac and Cabezas, Javier and Ramirez, Alex and Navarro, Nacho and Valero, Mateo},
	month = jun,
	year = {2014},
	pages = {193--204},
	file = {Full Text PDF:/Users/shinwookim/Zotero/storage/85YWH3SA/Tanasic et al. - 2014 - Enabling preemptive multiprogramming on GPUs.pdf:application/pdf;Tanasic et al. - 2014 - Enabling preemptive multiprogramming on GPUs.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Tanasic et al/Tanasic et al. - 2014 - Enabling preemptive multiprogramming on GPUs.pdf:application/pdf},
}

@inproceedings{molom-ochir_energy_2021,
	title = {Energy and {Cost} {Considerations} for {GPU} {Accelerated} {AI} {Inference} {Workloads}},
	url = {https://ieeexplore.ieee.org/abstract/document/9701614},
	doi = {10.1109/URTC54388.2021.9701614},
	abstract = {Recent advances in AI have motivated hardware manufacturers to design deep learning friendly accelerators to keep with the ever-growing increases in model sizes and compu-tational requirements. While early accelerators were utilized for model training, newer accelerators are capable of running deep neural network (DNN) model inferences and are increasingly used in robotics, vision, and edge applications. In this paper, we compare several popular embedded and desktop GPUs with respect to their performance and energy efficiency. Our results show that although larger devices always provide higher throughput, they are not always the most energy-efficient. GPUs vary in terms of their energy efficiency. To aid the process of hardware selection for a system designer, we use our experimental results to design a recommendation algorithm that chooses the ideal hardware accelerator under cost, power, and performance constraints.},
	urldate = {2024-01-18},
	booktitle = {2021 {IEEE} {MIT} {Undergraduate} {Research} {Technology} {Conference} ({URTC})},
	author = {Molom-Ochir, Tergel and Shenoy, Rohan},
	month = oct,
	year = {2021},
	pages = {1--5},
	file = {IEEE Xplore Abstract Record:/Users/shinwookim/Zotero/storage/BR2M5P6C/9701614.html:text/html;Molom-Ochir and Shenoy - 2021 - Energy and Cost Considerations for GPU Accelerated.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Molom-Ochir_Shenoy/Molom-Ochir and Shenoy - 2021 - Energy and Cost Considerations for GPU Accelerated.pdf:application/pdf},
}

@inproceedings{kim_study_2020,
	title = {A {Study} on {Energy}-{Process}-{Latency} {Tradeoff} in {Embedded} {Artificial} {Intelligence}},
	url = {https://ieeexplore.ieee.org/abstract/document/9289270},
	doi = {10.1109/ICTC49870.2020.9289270},
	abstract = {In this paper, we explore an impact of GPU/CPU scaling of a state-of-the-art AI embedded device on its energy consumption and AI performance. We use Nvidia Jetson TX2 as an experiment device thanks to its tractability to scale GPU/CPU and modify AI framework and libraries. Via extensive experiment in various ML (Machine Learning) scenarios, i.e., face recognition and objective detection, we demonstrate a clear tradeoff between GPU/CPU scaling, energy consumption (of GPU/CPU as well as entire device) and training/inference speed. Finally, we envision a future work aiming to optimize processing and networking resources simultaneously at an extended scenario that multiple AI embedded devices cooperate with each other for a common AI application.},
	urldate = {2024-01-18},
	booktitle = {2020 {International} {Conference} on {Information} and {Communication} {Technology} {Convergence} ({ICTC})},
	author = {Kim, Jinhwi and Galanopoulos, Apostolos and Vivek Joseph, Jude and Kwak, Jeongho},
	month = oct,
	year = {2020},
	note = {ISSN: 2162-1233},
	pages = {22--24},
	file = {Kim et al. - 2020 - A Study on Energy-Process-Latency Tradeoff in Embe.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Kim et al/Kim et al. - 2020 - A Study on Energy-Process-Latency Tradeoff in Embe.pdf:application/pdf},
}

@misc{patterson_carbon_2021,
	title = {Carbon {Emissions} and {Large} {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/2104.10350},
	doi = {10.48550/arXiv.2104.10350},
	abstract = {The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information. We calculate the energy use and carbon footprint of several recent large models-T5, Meena, GShard, Switch Transformer, and GPT-3-and refine earlier estimates for the neural architecture search that found Evolved Transformer. We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): Large but sparsely activated DNNs can consume {\textless}1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO2e vary {\textasciitilde}5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. Specific datacenter infrastructure matters, as Cloud datacenters can be {\textasciitilde}1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be {\textasciitilde}2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to {\textasciitilde}100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO2e explicit when practical. We are working to be more transparent about energy use and CO2e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO2e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
	month = apr,
	year = {2021},
	note = {arXiv:2104.10350 [cs]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/shinwookim/Zotero/storage/JUI6TY3A/2104.html:text/html;Patterson et al. - 2021 - Carbon Emissions and Large Neural Network Training.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Patterson et al/Patterson et al. - 2021 - Carbon Emissions and Large Neural Network Training.pdf:application/pdf},
}

@misc{anthony_carbontracker_2020,
	title = {Carbontracker: {Tracking} and {Predicting} the {Carbon} {Footprint} of {Training} {Deep} {Learning} {Models}},
	shorttitle = {Carbontracker},
	url = {http://arxiv.org/abs/2007.03051},
	doi = {10.48550/arXiv.2007.03051},
	abstract = {Deep learning (DL) can achieve impressive results across a wide variety of tasks, but this often comes at the cost of training models for extensive periods on specialized hardware accelerators. This energy-intensive workload has seen immense growth in recent years. Machine learning (ML) may become a significant contributor to climate change if this exponential trend continues. If practitioners are aware of their energy and carbon footprint, then they may actively take steps to reduce it whenever possible. In this work, we present Carbontracker, a tool for tracking and predicting the energy and carbon footprint of training DL models. We propose that energy and carbon footprint of model development and training is reported alongside performance metrics using tools like Carbontracker. We hope this will promote responsible computing in ML and encourage research into energy-efficient deep neural networks.},
	urldate = {2024-01-18},
	publisher = {arXiv},
	author = {Anthony, Lasse F. Wolff and Kanding, Benjamin and Selvan, Raghavendra},
	month = jul,
	year = {2020},
	note = {arXiv:2007.03051 [cs, eess, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
	file = {Anthony et al. - 2020 - Carbontracker Tracking and Predicting the Carbon .pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Anthony et al/Anthony et al. - 2020 - Carbontracker Tracking and Predicting the Carbon .pdf:application/pdf;arXiv.org Snapshot:/Users/shinwookim/Zotero/storage/L34HNE94/2007.html:text/html},
}

@inproceedings{chien_reducing_2023,
	address = {New York, NY, USA},
	series = {{HotCarbon} '23},
	title = {Reducing the {Carbon} {Impact} of {Generative} {AI} {Inference} (today and in 2035)},
	isbn = {9798400702426},
	url = {https://dl.acm.org/doi/10.1145/3604930.3605705},
	doi = {10.1145/3604930.3605705},
	abstract = {Generative AI, exemplified in ChatGPT, Dall-E 2, and Stable Diffusion, are exciting new applications consuming growing quantities of computing. We study the compute, energy, and carbon impacts of generative AI inference. Using ChatGPT as an exemplar, we create a workload model and compare request direction approaches (Local, Balance, CarbonMin), assessing their power use and carbon impacts. Our workload model shows that for ChatGPT-like services, inference dominates emissions, in one year producing 25x the carbon-emissions of training GPT-3. The workload model characterizes user experience, and experiments show that carbon emissions-aware algorithms (CarbonMin) can both maintain user experience and reduce carbon emissions dramatically (35\%). We also consider a future scenario (2035 workload and power grids), and show that CarbonMin can reduce emissions by 56\%. In both cases, the key is intelligent direction of requests to locations with low-carbon power. Combined with hardware technology advances, CarbonMin can keep emissions increase to only 20\% compared to 2022 levels for 55x greater workload. Finally we consider datacenter headroom to increase effectiveness of shifting. With headroom, CarbonMin reduces 2035 emissions by 71\%.},
	urldate = {2024-01-18},
	booktitle = {Proceedings of the 2nd {Workshop} on {Sustainable} {Computer} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Chien, Andrew A and Lin, Liuzixuan and Nguyen, Hai and Rao, Varsha and Sharma, Tristan and Wijayawardana, Rajini},
	month = aug,
	year = {2023},
	keywords = {carbon emissions, generative AI, geographic shifting, large language models, sustainability},
	pages = {1--7},
	file = {Chien et al. - 2023 - Reducing the Carbon Impact of Generative AI Infere.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Chien et al/Chien et al. - 2023 - Reducing the Carbon Impact of Generative AI Infere.pdf:application/pdf},
}

@misc{noauthor_survey_nodate,
	title = {A {Survey} of {Methods} for {Analyzing} and {Improving} {GPU} {Energy} {Efficiency} {\textbar} {ACM} {Computing} {Surveys}},
	url = {https://dl.acm.org/doi/abs/10.1145/2636342},
	urldate = {2024-01-18},
	file = {A Survey of Methods for Analyzing and Improving GPU Energy Efficiency | ACM Computing Surveys:/Users/shinwookim/Zotero/storage/YKVBXJKM/2636342.html:text/html},
}

@article{mittal_survey_2014,
	title = {A {Survey} of {Methods} for {Analyzing} and {Improving} {GPU} {Energy} {Efficiency}},
	volume = {47},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/2636342},
	doi = {10.1145/2636342},
	abstract = {Recent years have witnessed phenomenal growth in the computational capabilities and applications of GPUs. However, this trend has also led to a dramatic increase in their power consumption. This article surveys research works on analyzing and improving energy efficiency of GPUs. It also provides a classification of these techniques on the basis of their main research idea. Further, it attempts to synthesize research works that compare the energy efficiency of GPUs with other computing systems (e.g., FPGAs and CPUs). The aim of this survey is to provide researchers with knowledge of the state of the art in GPU power management and motivate them to architect highly energy-efficient GPUs of tomorrow.},
	number = {2},
	urldate = {2024-01-18},
	journal = {ACM Computing Surveys},
	author = {Mittal, Sparsh and Vetter, Jeffrey S.},
	month = aug,
	year = {2014},
	keywords = {architecture techniques, energy efficiency, energy saving, GPU (graphics-processing unit), green computing, power management, power model},
	pages = {19:1--19:23},
	file = {Full Text PDF:/Users/shinwookim/Zotero/storage/U9P8MLIQ/Mittal and Vetter - 2014 - A Survey of Methods for Analyzing and Improving GP.pdf:application/pdf},
}

@inproceedings{prakash_improving_2016,
	address = {New York, NY, USA},
	series = {{DAC} '16},
	title = {Improving mobile gaming performance through cooperative {CPU}-{GPU} thermal management},
	isbn = {978-1-4503-4236-0},
	url = {https://dl.acm.org/doi/10.1145/2897937.2898031},
	doi = {10.1145/2897937.2898031},
	abstract = {State-of-the-art thermal management techniques independently throttle the frequencies of high-performance multi-core CPU and powerful graphics processing units (GPU) on heterogeneous multiprocessor system-on-chips deployed in latest mobile devices. For graphics-intensive gaming applications, this approach is inadequate because both the CPU and the GPU contribute towards the overall application performance (frames per second or FPS) as well as the on-chip temperature. The lack of coordination between CPU and GPU induces recurrent frequency throttling to maintain on-chip temperature below the permissible limit. This leads to significantly degraded application performance and large variation in temperature over time. We propose a control-theory based dynamic thermal management technique that cooperatively scales CPU and GPU frequencies to meet the thermal constraint while achieving high performance for mobile gaming. Experimental results with six popular Android games on a commercial mobile platform show an average 19\% performance improvement and over 90\% reduction in temperature variance compared to the original Linux approach.},
	urldate = {2024-01-18},
	booktitle = {Proceedings of the 53rd {Annual} {Design} {Automation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Prakash, Alok and Amrouch, Hussam and Shafique, Muhammad and Mitra, Tulika and Henkel, Jörg},
	month = jun,
	year = {2016},
	pages = {1--6},
	file = {Full Text PDF:/Users/shinwookim/Zotero/storage/L949NXU6/Prakash et al. - 2016 - Improving mobile gaming performance through cooper.pdf:application/pdf},
}

@article{wang_optic_2019,
	title = {{OPTiC}: {Optimizing} {Collaborative} {CPU}–{GPU} {Computing} on {Mobile} {Devices} {With} {Thermal} {Constraints}},
	volume = {38},
	issn = {1937-4151},
	shorttitle = {{OPTiC}},
	url = {https://ieeexplore.ieee.org/abstract/document/8477038},
	doi = {10.1109/TCAD.2018.2873210},
	abstract = {The CPU-graphic processing unit (GPU) co-execution of computation kernels on heterogeneous multiprocessor system-on-chip can significantly boost performance compared to the execution on either the CPU or the GPU alone. However, engaging multiple on-chip compute elements concurrently at the highest frequency may not provide the optimal performance in a mobile system with stringent thermal constraints. The system may repeatedly exceed the temperature threshold necessitating frequency throttling and hence performance degradation. We present OPTiC, an analytical framework that given a computation kernel can automatically select the partitioning point and the operating frequencies for optimal CPU-GPU co-execution under thermal constraints. OPTiC estimates, through modeling, CPU and GPU power, performance at different frequency points as well as the performance impact of thermal throttling and memory contention. Experimental evaluation on a commercial mobile platform shows that OPTiC achieves an average 13.68\% performance improvement over existing schemes that enable co-execution without thermal considerations.},
	number = {3},
	urldate = {2024-01-18},
	journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	author = {Wang, Siqi and Ananthanarayanan, Gayathri and Mitra, Tulika},
	month = mar,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
	pages = {393--406},
	file = {IEEE Xplore Abstract Record:/Users/shinwookim/Zotero/storage/4D753ZFH/8477038.html:text/html;IEEE Xplore Full Text PDF:/Users/shinwookim/Zotero/storage/A28EV2EI/Wang et al. - 2019 - OPTiC Optimizing Collaborative CPU–GPU Computing .pdf:application/pdf},
}

@inproceedings{prakash_energy-efficient_2015,
	title = {Energy-efficient execution of data-parallel applications on heterogeneous mobile platforms},
	url = {https://ieeexplore.ieee.org/document/7357105},
	doi = {10.1109/ICCD.2015.7357105},
	abstract = {State-of-the-art mobile system-on-chips (SoC) include heterogeneity in various forms for accelerated and energy-efficient execution of diverse range of applications. The modern SoCs now include programmable cores such as CPU and GPU with very different functionality. The SoCs also integrate performance heterogeneous cores with different power-performance characteristics but the same instruction-set architecture such as ARM big.LITTLE. In this paper, we first explore and establish the combined benefits of functional heterogeneity and performance heterogeneity in improving power-performance behavior of data parallel applications. Next, given an application specified in OpenCL, we present a static partitioning strategy to execute the application kernel across CPU and GPU cores along with voltage-frequency setting for individual cores so as to obtain the best power-performance tradeoff. We achieve over 19\% runtime improvement by exploiting the functional and performance heterogeneities concurrently. In addition, energy saving of 36\% is achieved by using appropriate voltage-frequency setting without significantly degrading the runtime improvement from concurrent execution.},
	urldate = {2024-01-25},
	booktitle = {2015 33rd {IEEE} {International} {Conference} on {Computer} {Design} ({ICCD})},
	author = {Prakash, Alok and Wang, Siqi and Irimiea, Alexandru Eugen and Mitra, Tulika},
	month = oct,
	year = {2015},
	keywords = {Graphics processing units, Kernel, Mobile communication, Multicore processing, Pipelines, Runtime},
	pages = {208--215},
	file = {IEEE Xplore Abstract Record:/Users/shinwookim/Zotero/storage/9C8P6DRS/7357105.html:text/html;Prakash et al. - 2015 - Energy-efficient execution of data-parallel applic.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Prakash et al/Prakash et al. - 2015 - Energy-efficient execution of data-parallel applic.pdf:application/pdf},
}

@inproceedings{wen_merge_2017,
	address = {New York, NY, USA},
	series = {{GPGPU}-10},
	title = {Merge or {Separate}? {Multi}-job {Scheduling} for {OpenCL} {Kernels} on {CPU}/{GPU} {Platforms}},
	isbn = {978-1-4503-4915-4},
	shorttitle = {Merge or {Separate}?},
	url = {https://dl.acm.org/doi/10.1145/3038228.3038235},
	doi = {10.1145/3038228.3038235},
	abstract = {Computer systems are increasingly heterogeneous with nodes consisting of CPUs and GPU accelerators. As such systems become mainstream, they move away from specialized high-performance single application platforms to a more general setting with multiple, concurrent, application jobs. Determining how jobs should be dynamically best scheduled to heterogeneous devices is non-trivial. In certain cases, performance is maximized if jobs are allocated to a single device, in others, sharing is preferable. In this paper, we present a runtime framework which schedules multi-user OpenCL tasks to their most suitable device in a CPU/GPU system. We use a machine learning-based predictive model at runtime to detect whether to merge OpenCL kernels or schedule them separately to the most appropriate devices without the need for ahead-of-time profiling. We evaluate out approach over a wide range of workloads, on two separate platforms. We consistently show significant performance and turn-around time improvement over the state-of-the-art across programs, workload, and platforms.},
	urldate = {2024-01-24},
	booktitle = {Proceedings of the {General} {Purpose} {GPUs}},
	publisher = {Association for Computing Machinery},
	author = {Wen, Yuan and O'Boyle, Michael F.P.},
	month = feb,
	year = {2017},
	keywords = {machine learning, concurrent kernel, CPU-GPU runtime system, GPU kernel scheduling},
	pages = {22--31},
	file = {Wen and O'Boyle - 2017 - Merge or Separate Multi-job Scheduling for OpenCL.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Wen_O'Boyle/Wen and O'Boyle - 2017 - Merge or Separate Multi-job Scheduling for OpenCL.pdf:application/pdf},
}

@article{singh_energy-efficient_2017,
	title = {Energy-{Efficient} {Run}-{Time} {Mapping} and {Thread} {Partitioning} of {Concurrent} {OpenCL} {Applications} on {CPU}-{GPU} {MPSoCs}},
	volume = {16},
	issn = {1539-9087},
	url = {https://dl.acm.org/doi/10.1145/3126548},
	doi = {10.1145/3126548},
	abstract = {Heterogeneous Multi-Processor Systems-on-Chips (MPSoCs) containing CPU and GPU cores are typically required to execute applications concurrently. However, as will be shown in this paper, existing approaches are not well suited for concurrent applications as they are developed either by considering only a single application or they do not exploit both CPU and GPU cores at the same time. In this paper, we propose an energy-efficient run-time mapping and thread partitioning approach for executing concurrent OpenCL applications on both GPU and GPU cores while satisfying performance requirements. Depending upon the performance requirements, for each concurrently executing application, the mapping process finds the appropriate number of CPU cores and operating frequencies of CPU and GPU cores, and the partitioning process identifies an efficient partitioning of the applications’ threads between CPU and GPU cores. We validate the proposed approach experimentally on the Odroid-XU3 hardware platform with various mixes of applications from the Polybench benchmark suite. Additionally, a case-study is performed with a real-world application SLAMBench. Results show an average energy saving of 32\% compared to existing approaches while still satisfying the performance requirements.},
	number = {5s},
	urldate = {2024-01-25},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Singh, Amit Kumar and Prakash, Alok and Basireddy, Karunakar Reddy and Merrett, Geoff V. and Al-Hashimi, Bashir M.},
	month = sep,
	year = {2017},
	keywords = {Energy consumption, Heterogeneous MPSoC, OpenCL applications, Performance, Run-time management},
	pages = {147:1--147:22},
	file = {Singh et al. - 2017 - Energy-Efficient Run-Time Mapping and Thread Parti.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Singh et al/Singh et al. - 2017 - Energy-Efficient Run-Time Mapping and Thread Parti.pdf:application/pdf},
}

@inproceedings{kerr_modeling_2010,
	address = {New York, NY, USA},
	series = {{GPGPU}-3},
	title = {Modeling {GPU}-{CPU} workloads and systems},
	isbn = {978-1-60558-935-0},
	url = {https://dl.acm.org/doi/10.1145/1735688.1735696},
	doi = {10.1145/1735688.1735696},
	abstract = {Heterogeneous systems, systems with multiple processors tailored for specialized tasks, are challenging programming environments. While it may be possible for domain experts to optimize a high performance application for a very specific and well documented system, it may not perform as well or even function on a different system. Developers who have less experience with either the application domain or the system architecture may devote a significant effort to writing a program that merely functions correctly. We believe that a comprehensive analysis and modeling frame-work is necessary to ease application development and automate program optimization on heterogeneous platforms. This paper reports on an empirical evaluation of 25 CUDA applications on four GPUs and three CPUs, leveraging the Ocelot dynamic compiler infrastructure which can execute and instrument the same CUDA applications on either target. Using a combination of instrumentation and statistical analysis, we record 37 different metrics for each application and use them to derive relationships between program behavior and performance on heterogeneous processors. These relationships are then fed into a modeling frame-work that attempts to predict the performance of similar classes of applications on different processors. Most significantly, this study identifies several non-intuitive relationships between program characteristics and demonstrates that it is possible to accurately model CUDA kernel performance using only metrics that are available before a kernel is executed.},
	urldate = {2024-01-25},
	booktitle = {Proceedings of the 3rd {Workshop} on {General}-{Purpose} {Computation} on {Graphics} {Processing} {Units}},
	publisher = {Association for Computing Machinery},
	author = {Kerr, Andrew and Diamos, Gregory and Yalamanchili, Sudhakar},
	month = mar,
	year = {2010},
	keywords = {CUDA, GPGPU, Ocelot, OpenCL, parboil, PTX, Rodinia},
	pages = {31--42},
	file = {Kerr et al. - 2010 - Modeling GPU-CPU workloads and systems.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Kerr et al/Kerr et al. - 2010 - Modeling GPU-CPU workloads and systems.pdf:application/pdf},
}

@article{lee_thermal-aware_2019,
	title = {Thermal-{Aware} {Scheduling} for {Integrated} {CPUs}--{GPU} {Platforms}},
	volume = {18},
	issn = {1539-9087},
	url = {https://dl.acm.org/doi/10.1145/3358235},
	doi = {10.1145/3358235},
	abstract = {As modern embedded systems like cars need high-power integrated CPUs--GPU SoCs for various real-time applications such as lane or pedestrian detection, they face greater thermal problems than before, which may, in turn, incur higher failure rate and cooling cost. We demonstrate, via experimentation on a representative CPUs--GPU platform, the importance of accounting for two distinct thermal characteristics—the platform’s temperature imbalance and different power dissipations of different tasks—in real-time scheduling to avoid any burst of power dissipations while guaranteeing all timing constraints. To achieve this goal, we propose a new {\textless}u{\textgreater}R{\textless}/u{\textgreater}eal-{\textless}u{\textgreater}T{\textless}/u{\textgreater}ime {\textless}u{\textgreater}T{\textless}/u{\textgreater}hermal-{\textless}u{\textgreater}A{\textless}/u{\textgreater}ware {\textless}u{\textgreater}S{\textless}/u{\textgreater}cheduling (RT-TAS) framework. We first capture different CPU cores’ temperatures caused by different GPU power dissipations (i.e., CPUs--GPU thermal coupling) with core-specific thermal coupling coefficients. We then develop thermally-balanced task-to-core assignment and CPUs--GPU co-scheduling. The former addresses the platform’s temperature imbalance by efficiently distributing the thermal load across cores while preserving scheduling feasibility. Building on the thermally-balanced task assignment, the latter cooperatively schedules CPU and GPU computations to avoid simultaneous peak power dissipations on both CPUs and GPU, thus mitigating excessive temperature rises while meeting task deadlines. We have implemented and evaluated RT-TAS on an automotive embedded platform to demonstrate its effectiveness in reducing the maximum temperature by 6−12.2°C over existing approaches without violating any task deadline.},
	number = {5s},
	urldate = {2024-02-01},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Lee, Youngmoon and Shin, Kang G. and Chwa, Hoon Sung},
	month = oct,
	year = {2019},
	keywords = {embedded systems, GPU, real-time systems, Thermal management},
	pages = {90:1--90:25},
	file = {Lee et al. - 2019 - Thermal-Aware Scheduling for Integrated CPUs--GPU .pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Lee et al/Lee et al. - 2019 - Thermal-Aware Scheduling for Integrated CPUs--GPU .pdf:application/pdf},
}

@article{ahmed_design_2017,
	title = {On {The} {Design} and {Application} of {Thermal} {Isolation} {Servers}},
	volume = {16},
	issn = {1539-9087},
	url = {https://dl.acm.org/doi/10.1145/3126512},
	doi = {10.1145/3126512},
	abstract = {Recently, there has been an increasing trend towards executing real-time applications on multi-core platforms. However, this complicates the design problem, as applications running on different cores can interfere due to shared resources and mediums. In this paper, we focus on thermal interference, where a given task (τ 1) heats the processor, resulting in reduced service (due to Dynamic Thermal Management (DTM)) to another task (τ2). In real-time domain, where tasks have deadline constraints, thermal interference is a substantial problem as it directly impacts the Worst Case Execution Time (WCET) of the effected application (τ2). The problem exacerbates as we move to mixed-criticality systems, where the criticality of τ2 may be greater than the criticality of τ1, complicating the certification process. In this paper, we propose a server based strategy (Thermal Isolation Server (TI Server)) which can be used to avoid thermal interference of applications. We also present a heuristic to design TI Servers to meet the timing constraints of all tasks and the thermal constraints of the system. TI Servers are time/space composable, and can be applied to a variety of task models. We also evaluate TI Servers on a hardware test-bed for validation purposes.},
	number = {5s},
	urldate = {2024-02-01},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Ahmed, Rehan and Huang, Pengcheng and Millen, Max and Thiele, Lothar},
	month = sep,
	year = {2017},
	keywords = {Mixed-criticality systems, Thermal modeling},
	pages = {165:1--165:19},
	file = {Ahmed et al. - 2017 - On The Design and Application of Thermal Isolation.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Ahmed et al/Ahmed et al. - 2017 - On The Design and Application of Thermal Isolation.pdf:application/pdf},
}

@article{stern_its_2023,
	chapter = {Tech},
	title = {It’s {Not} {Just} {You}, {Heat} {Is} {Making} {Our} {Smartphone} {Batteries} {Worse}. {Here}’s {Why}.},
	issn = {0099-9660},
	url = {https://www.wsj.com/articles/i-tried-to-melt-a-bunch-of-smartphones-to-find-out-how-you-can-save-your-battery-d75066d1},
	abstract = {Heat is killing your battery. Our columnist provides some tips on how to protect your iPhone or Galaxy S.},
	language = {en-US},
	urldate = {2024-02-01},
	journal = {Wall Street Journal},
	author = {Stern, Joanna},
	month = aug,
	year = {2023},
	keywords = {005930.SE, AAPL, Advice, Apple, Batteries, cell, Cell/Mobile Phones, computers, Computers/Consumer Electronics, Consumer Electronics, Content Types, general news, Industrial Electronics, Industrial Goods, KR:005930, lifestyle, living, Living/Lifestyle, Mobile Devices, mobile phones, North America, Personal Electronics, Personal Technology, political, Political/General News, Samsung, Samsung Electronics, SYND, Technology, Telecommunications Equipment, United States, WSJ-PRO-WSJ.com, wsjlifework},
	file = {Snapshot:/Users/shinwookim/Zotero/storage/HGRITIAM/i-tried-to-melt-a-bunch-of-smartphones-to-find-out-how-you-can-save-your-battery-d75066d1.html:text/html},
}

@article{leng_effect_2015,
	title = {Effect of {Temperature} on the {Aging} rate of {Li} {Ion} {Battery} {Operating} above {Room} {Temperature}},
	volume = {5},
	copyright = {2015 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep12967},
	doi = {10.1038/srep12967},
	abstract = {Temperature is known to have a significant impact on the performance, safety and cycle lifetime of lithium-ion batteries (LiB). However, the comprehensive effects of temperature on the cyclic aging rate of LiB have yet to be found. We use an electrochemistry-based model (ECBE) here to measure the effects on the aging behavior of cycled LiB operating within the temperature range of 25 °C to 55 °C. The increasing degradation rate of the maximum charge storage of LiB during cycling at elevated temperature is found to relate mainly to the degradations at the electrodes and that the degradation of LCO cathode is larger than graphite anode at elevated temperature. In particular, the formation and modification of the surface films on the electrodes as well as structural/phase changes of the LCO electrode, as reported in the literatures, are found to be the main contributors to the increasing degradation rate of the maximum charge storage of LiB with temperature for the specific operating temperature range. Larger increases in the Warburg elements and cell impedance are also found with cycling at higher temperature, but they do not seriously affect the state of health (SoH) of LiB as shown in this work.},
	language = {en},
	number = {1},
	urldate = {2024-02-01},
	journal = {Scientific Reports},
	author = {Leng, Feng and Tan, Cher Ming and Pecht, Michael},
	month = aug,
	year = {2015},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Batteries, Electrical and electronic engineering, Electrochemistry, Electronics, photonics and device physics},
	pages = {12967},
	file = {Full Text PDF:/Users/shinwookim/Zotero/storage/2F9NS3SQ/Leng et al. - 2015 - Effect of Temperature on the Aging rate of Li Ion .pdf:application/pdf;Leng et al. - 2015 - Effect of Temperature on the Aging rate of Li Ion .pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Leng et al/Leng et al. - 2015 - Effect of Temperature on the Aging rate of Li Ion .pdf:application/pdf},
}

@misc{bach_impact_2014,
	title = {Impact of {Temperature} on {Intel} {CPU} {Performance}},
	url = {https://www.pugetsystems.com/labs/articles/impact-of-temperature-on-intel-cpu-performance-606/},
	abstract = {While we all know that modern processors need active cooling, there is actually very little official information on how temperature affects a CPU's performance. Do you really need a high-end liquid cooled setup to get peak performance, or is the little stock cooler that comes with most CPUs enough? In this article we will examine exactly how temperature affects CPU performance.},
	language = {en-US},
	urldate = {2024-02-01},
	journal = {Puget Systems},
	author = {Bach, Matt},
	month = oct,
	year = {2014},
	file = {Snapshot:/Users/shinwookim/Zotero/storage/6XY3I646/impact-of-temperature-on-intel-cpu-performance-606.html:text/html},
}

@article{haywood_relationship_2015,
	title = {The relationship among {CPU} utilization, temperature, and thermal power for waste heat utilization},
	volume = {95},
	issn = {0196-8904},
	url = {https://www.sciencedirect.com/science/article/pii/S0196890415001028},
	doi = {10.1016/j.enconman.2015.01.088},
	abstract = {This work addresses significant datacenter issues of growth in numbers of computer servers and subsequent electricity expenditure by proposing, analyzing and testing a unique idea of recycling the highest quality waste heat generated by datacenter servers. The aim was to provide a renewable and sustainable energy source for use in cooling the datacenter. The work incorporates novel approaches in waste heat usage, graphing CPU temperature, power and utilization simultaneously, and a mineral oil experimental design and implementation. The work presented investigates and illustrates the quantity and quality of heat that can be captured from a variably tasked liquid-cooled microprocessor on a datacenter server blade. It undertakes a radical approach using mineral oil. The trials examine the feasibility of using the thermal energy from a CPU to drive a cooling process. Results indicate that 123 servers encapsulated in mineral oil can power a 10-ton chiller with a design point of 50.2kWth. Compared with water-cooling experiments, the mineral oil experiment mitigated the temperature drop between the heat source and discharge line by up to 81\%. In addition, due to this reduction in temperature drop, the heat quality in the oil discharge line was up to 12.3°C higher on average than for water-cooled experiments. Furthermore, mineral oil cooling holds the potential to eliminate the 50\% cooling expenditure which initially motivated this project.},
	urldate = {2024-02-01},
	journal = {Energy Conversion and Management},
	author = {Haywood, Anna M. and Sherbeck, Jon and Phelan, Patrick and Varsamopoulos, Georgios and Gupta, Sandeep K. S.},
	month = may,
	year = {2015},
	keywords = {Absorption chiller, CPU heat, power and temperature simultaneously, Data center waste heat, Heat-extraction, Liquid cooling, Waste heat reuse},
	pages = {297--303},
	file = {ScienceDirect Snapshot:/Users/shinwookim/Zotero/storage/VV8W3GS8/S0196890415001028.html:text/html},
}

@article{noauthor_how_2020,
	title = {How to {Boost} {Your} {CPU}, {GPU}, and {SoC} {Performance} {Through} {Thermal} {Accuracy}},
	abstract = {Whether on a smartphone, Personal Computer (PC), or an everyday laptop notebook, it is important to provide maximum processor performance. Technology is advancing quickly every day, and the need for faster processors is becoming even more important. The processing power of your device is typically determined by the Central Processing Unit (CPU) and Graphic Processing Unit (GPU). A major limiting factor of CPUs and GPUs are their thermal design point.},
	language = {en},
	year = {2020},
	file = {2020 - How to Boost Your CPU, GPU, and SoC Performance Th.pdf:/Users/shinwookim/Zotero/storage/WEPM52KY/2020 - How to Boost Your CPU, GPU, and SoC Performance Th.pdf:application/pdf},
}

@misc{maitra_boosting_2023,
	title = {Boosting {CPU}, {GPU}, {SoC} {Performance} through {Precise} {Thermal} {Management}},
	url = {https://arnabmaitra.medium.com/boosting-cpu-gpu-soc-performance-through-precise-thermal-management-390ca3dc9c3a},
	abstract = {In the fast-paced world of technology, achieving maximum processor performance is crucial for an exceptional user experience. The heart of…},
	language = {en},
	urldate = {2024-02-01},
	journal = {Medium},
	author = {Maitra, Arnab},
	month = jul,
	year = {2023},
	file = {Snapshot:/Users/shinwookim/Zotero/storage/9HYSIWKZ/boosting-cpu-gpu-soc-performance-through-precise-thermal-management-390ca3dc9c3a.html:text/html},
}

@article{cao_prediction_2023,
	title = {Prediction of the {Heat} {Generation} {Rate} of {Lithium}-{Ion} {Batteries} {Based} on {Three} {Machine} {Learning} {Algorithms}},
	volume = {9},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2313-0105},
	url = {https://www.mdpi.com/2313-0105/9/3/165},
	doi = {10.3390/batteries9030165},
	abstract = {The heat generation rate (HGR) of lithium-ion batteries is crucial for the design of a battery thermal management system. Machine learning algorithms can effectively solve nonlinear problems and have been implemented in the state estimation and life prediction of batteries; however, limited research has been conducted on determining the battery HGR through machine learning. In this study, we employ three common machine learning algorithms, i.e., artificial neural network (ANN), support vector machine (SVM), and Gaussian process regression (GPR), to predict the battery HGR based on our experimental data, along with cases of interpolation and extrapolation. The results indicated the following: (1) the prediction accuracies for the interpolation cases were better than those of extrapolation, and the R2 values of interpolation were greater than 0.96; (2) after the discharge voltage was added as an input parameter, the prediction of the ANN was barely affected, whereas the performance of the SVM and GPR were improved; and (3) the ANN exhibited the best performance among the three algorithms. Accurate results can be obtained by using a single hidden layer and no more than 15 neurons without the additional input, where the R2 values were in the range of 0.89–1.00. Therefore, the ANN is preferable for predicting the HGR of lithium-ion batteries.},
	language = {en},
	number = {3},
	urldate = {2024-02-01},
	journal = {Batteries},
	author = {Cao, Renfeng and Zhang, Xingjuan and Yang, Han},
	month = mar,
	year = {2023},
	note = {Number: 3
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {artificial neural network, Gaussian process regression, heat generation rate, lithium-ion battery, machine learning, support vector machine},
	pages = {165},
	file = {Full Text PDF:/Users/shinwookim/Zotero/storage/YS6WIK6C/Cao et al. - 2023 - Prediction of the Heat Generation Rate of Lithium-.pdf:application/pdf},
}

@misc{noauthor_accelerating_nodate,
	title = {Accelerating generative {AI} at the edge},
	url = {https://www.qualcomm.com/news/onq/2023/11/accelerating-generative-ai-at-the-edge},
	abstract = {Knowledge distillation and quantization, combined with full-stack optimizations by Qualcomm AI Research, make on-device generative AI experiences fast and efficient.},
	language = {en},
	urldate = {2024-02-15},
	file = {Snapshot:/Users/shinwookim/Zotero/storage/8G3IP4UJ/accelerating-generative-ai-at-the-edge.html:text/html},
}

@misc{he_rest_2023,
	title = {{REST}: {Retrieval}-{Based} {Speculative} {Decoding}},
	shorttitle = {{REST}},
	url = {http://arxiv.org/abs/2311.08252},
	doi = {10.48550/arXiv.2311.08252},
	abstract = {We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm designed to speed up language model generation. The key insight driving the development of REST is the observation that the process of text generation often includes certain common phases and patterns. Unlike previous methods that rely on a draft language model for speculative decoding, REST harnesses the power of retrieval to generate draft tokens. This method draws from the reservoir of existing knowledge, retrieving and employing relevant tokens based on the current context. Its plug-and-play nature allows for seamless integration and acceleration of any language models, all without necessitating additional training. When benchmarked on 7B and 13B language models in a single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on code or text generation. The code of REST is available at https://github.com/FasterDecoding/REST.},
	urldate = {2024-02-21},
	publisher = {arXiv},
	author = {He, Zhenyu and Zhong, Zexuan and Cai, Tianle and Lee, Jason D. and He, Di},
	month = nov,
	year = {2023},
	note = {arXiv:2311.08252 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/shinwookim/Zotero/storage/CGBZNISQ/2311.html:text/html;He et al. - 2023 - REST Retrieval-Based Speculative Decoding.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/He et al/He et al. - 2023 - REST Retrieval-Based Speculative Decoding.pdf:application/pdf},
}

@misc{yan_decoding_2024,
	title = {Decoding {Speculative} {Decoding}},
	url = {http://arxiv.org/abs/2402.01528},
	doi = {10.48550/arXiv.2402.01528},
	abstract = {Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be used to decide the right draft model for a given workload. Further, using our insights we design a new draft model for LLaMA-65B which can provide 30\% higher throughput than existing draft models.},
	urldate = {2024-03-02},
	publisher = {arXiv},
	author = {Yan, Minghao and Agarwal, Saurabh and Venkataraman, Shivaram},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01528 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/shinwookim/Zotero/storage/7ZQ7NTID/2402.html:text/html;Yan et al. - 2024 - Decoding Speculative Decoding.pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Yan et al/Yan et al. - 2024 - Decoding Speculative Decoding.pdf:application/pdf},
}

@misc{miao_specinfer_2024,
	title = {{SpecInfer}: {Accelerating} {Generative} {Large} {Language} {Model} {Serving} with {Tree}-based {Speculative} {Inference} and {Verification}},
	shorttitle = {{SpecInfer}},
	url = {http://arxiv.org/abs/2305.09781},
	doi = {10.48550/arXiv.2305.09781},
	abstract = {This paper introduces SpecInfer, a system that accelerates generative large language model (LLM) serving with tree-based speculative inference and verification. The key idea behind SpecInfer is leveraging small speculative models to predict the LLM's outputs; the predictions are organized as a token tree, whose nodes each represent a candidate token sequence. The correctness of all candidate token sequences represented by a token tree is verified against the LLM in parallel using a novel tree-based parallel decoding mechanism. SpecInfer uses an LLM as a token tree verifier instead of an incremental decoder, which significantly reduces the end-to-end latency and computational requirement for serving generative LLMs while provably preserving model quality. Our evaluation shows that SpecInfer outperforms existing LLM serving systems by 1.5-2.8x for distributed LLM inference and by 2.6-3.5x for offloading-based LLM inference, while preserving the same generative performance. SpecInfer is publicly available at https://github.com/flexflow/FlexFlow/},
	urldate = {2024-03-21},
	publisher = {arXiv},
	author = {Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and Shi, Chunan and Chen, Zhuoming and Arfeen, Daiyaan and Abhyankar, Reyna and Jia, Zhihao},
	month = jan,
	year = {2024},
	note = {arXiv:2305.09781 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Distributed, Parallel, and Cluster Computing},
	file = {arXiv.org Snapshot:/Users/shinwookim/Zotero/storage/DYGHMVRB/2305.html:text/html;Miao et al. - 2024 - SpecInfer Accelerating Generative Large Language .pdf:/Users/shinwookim/Library/CloudStorage/OneDrive-UniversityofPittsburgh/zotero-library/Miao et al/Miao et al. - 2024 - SpecInfer Accelerating Generative Large Language .pdf:application/pdf},
}
